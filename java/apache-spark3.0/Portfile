# -*- coding: utf-8; mode: tcl; tab-width: 4; indent-tabs-mode: nil; c-basic-offset: 4 -*- vim:fenc=utf-8:ft=tcl:et:sw=4:ts=4:sts=4
PortSystem          1.0
PortGroup           java 1.0
PortGroup           select 1.0

name                apache-spark3.0
version             3.0.1
revision            0
categories          java parallel devel lang databases
maintainers         nomaintainer
description         Engine for large-scale data processing
long_description \
    Apache Spark is a lightning-fast unified analytics engine for big data and machine \
    learning. It was originally developed at UC Berkeley in 2009. Apache Spark is a fast \
    and general-purpose cluster computing system. It provides high-level APIs in Java, \
    Scala and Python, and an optimized engine that supports general execution graphs. \
    It also supports a rich set of higher-level tools including Spark SQL for SQL and \
    structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.
homepage            https://spark.apache.org/
platforms           darwin
supported_archs     noarch
license             Apache-2
use_xcode           no

master_sites        http://apache.mirrors.hoobly.com/spark/spark-${version}/ \
                    http://apache.mirrors.pair.com/spark/spark-${version}/ \
                    http://apache.spinellicreations.com/spark/spark-${version}/ \
                    http://mirror.cc.columbia.edu/pub/software/apache/spark/spark-${version}/ \
                    http://mirror.cogentco.com/pub/apache/spark/spark-${version}/ \
                    http://mirror.metrocast.net/apache/spark/spark-${version}/ \
                    http://mirrors.advancedhosters.com/apache/spark/spark-${version}/ \
                    http://mirrors.ibiblio.org/apache/spark/spark-${version}/ \
                    http://www.gtlib.gatech.edu/pub/apache/spark/spark-${version}/ \
                    http://www.trieuvan.com/apache/spark/spark-${version}/ \
                    https://apache.claz.org/spark/spark-${version}/ \
                    https://apache.cs.utah.edu/spark/spark-${version}/ \
                    https://apache.mirrors.lucidnetworks.net/spark/spark-${version}/ \
                    https://apache.osuosl.org/spark/spark-${version}/ \
                    https://ftp.wayne.edu/apache/spark/spark-${version}/ \
                    https://mirror.olnevhost.net/pub/apache/spark/spark-${version}/ \
                    https://mirrors.gigenet.com/apache/spark/spark-${version}/ \
                    https://mirrors.koehn.com/apache/spark/spark-${version}/ \
                    https://mirrors.ocf.berkeley.edu/apache/spark/spark-${version}/ \
                    https://mirrors.sonic.net/apache/spark/spark-${version}/ \
                    https://us.mirrors.quenda.co/apache/spark/spark-${version}/ \
                    https://archive.apache.org/dist/spark/spark-${version}/
distname            spark-${version}-bin-hadoop3.2
distfiles           ${distname}.tgz
extract.suffix      .tgz
checksums           md5    31e019e35e75a4c55c7efa4464641bf1\
                    sha1   6ed7e4e1ba71e4d1ea2d113c4df85acdb8ec124c\
                    rmd160 3ca0876dbafd66b6948333f3621b70df58bf4f80\
                    sha256 e2d05efa1c657dd5180628a83ea36c97c00f972b4aee935b7affa2e1058b0279\
                    size   224062525

use_configure       no

# Require java version
java.version        11+
# JDK port to install if required java not found
java.fallback       openjdk11

depends_run         port:sbt port:scala2.13 port:python38

extract.suffix      .tgz
extract.mkdir       yes

if {$subport == $name} {
    depends_lib          port:apache-spark_select
    select.group         apache-spark
    select.file          ${filespath}/${name}

    build               { }

    destroot {
        set sparkdir ${prefix}/share/${name}

        xinstall -m 755 -d ${sparkdir}
    
    	file copy ${worksrcpath}/${distname} ${destroot}${sparkdir}
	
	    set beeline.cmd beeline
    	set docker-image-tool.cmd docker-image-tool.sh
	    set find-spark-home.cmd find-spark-home
    	set load-spark-env.cmd load-spark-env.sh
    	set pyspark.cmd pyspark
	    set spark-class.cmd spark-class
    	set spark-shell.cmd spark-shell
	    set spark-sql.cmd spark-sql
    	set spark-submit.cmd spark-submit
	    set sparkR.cmd sparkR
	
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${beeline.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${docker-image-tool.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${find-spark-home.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${load-spark-env.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${pyspark.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-class.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-shell.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-sql.cmd} ${destroot}${sparkdir}/bin
    	xinstall -m 755 ${worksrcpath}/${distname}/bin/${spark-submit.cmd} ${destroot}${sparkdir}/bin
	    xinstall -m 755 ${worksrcpath}/${distname}/bin/${sparkR.cmd} ${destroot}${sparkdir}/bin
    }
}

notes "
The Spark Home environment variable requires the environment variables to be exported and
set as well as the PySpark Python version. Please add the following lines to your .profile
or export in your shell session to make ${name} work:

    export SPARK_HOME=${prefix}/share/${name}
    export PYSPARK_PYTHON=python3
"
